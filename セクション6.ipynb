{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本セクションの目次"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kafka / Spark Streaming / Avro\n",
    "kafka / Spark Streaming / Avro はストリーミングシステムを作成する上での3種の神器です。\n",
    "kafka と Spark Stramingについては前セクションで紹介しましたが、今回はそれに加えてAvroフォーマットについて学んでいきましょう\n",
    "\n",
    "## Avro フォーマットとは？\n",
    "\n",
    "もう一つがHadoopの生みの親であるDoug Cutting氏によりプロジェクト化されたAvro（アブロ）フォーマットです【URL】https://avro.apache.org。Avroフォーマットはおもにストリーミングでのやり取りで効力を発揮するフォーマットです余談ですが、同様のしくみとしてプロトコルバッファー（Protocol Buffers）は有名です。　【URL】https://developers.google.com/protocol-buffers/。 　元々AvroはHadoopの弱点であったJavaでしか読み書きできないという言語のポータビリティを解決するために生まれました言語のポータビリティーが低いということはそのままAvroファイルと連携する対向のシステムの利用言語まで縛ってしまう可能性があります。。 　Avroフォーマットの特徴は以下です。 ＠＠＠＠＠半行空き ・行指向フォーマット ・前方互換性と、後方互換性、完全互換を持ち複数のシステム間で速度の違う開発を行うことが可能 ・スキーマエボリューションを提供する ・Parquetに比べてJSONのようなリッチなフォーマットを表現可能\n",
    "\n",
    "## スキーマファイルを保存する場所\n",
    "スキーマファイルを保存する場所をスキーマレジストリと呼んだりします。\n",
    "スキーマレジストリとは、共有してファイルを読み取れる場所である必要があります。\n",
    "\n",
    "有名な企業としてConfulentが存在しますが、ただし今回はschemaのディレクトリをスキーマレジストリとします。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avroにおける前方互換、後方互換、完全互換\n",
    "\n",
    "さまざまな言語で利用可能になった今、さらに注目されているAvroの特徴が開発スピードの違いを吸収することができるということです。一般にデータ基盤が相手をするシステムはスモールデータシステム含め社内のシステムすべてです。そのシステム群の開発のスピードを合わせようと思ったら組織が大きくなるにつれて調整のコストが増大し調整自体が不可能に近くなります。 　そこで、後方互換性や前方互換性というしくみが活躍します。後方互換性とは、新しい製品が、古い製品を扱えることを指します。前方互換性とは、古い製品が新しい製品を扱えることを指します。たとえば、Excel2010がExcel2003を扱えるようにすることを後方互換。Excel2003がExcel2010を扱えるようにすることを前方互換。ということを指します。 　後方互換や前方互換の機能を利用することによって、一方のシステムへ変更があったときでも、ほかのシステムの稼働を維持しつつ自システムの変更を行うことができるのです。このようなしくみを提供することをスキーマエボリューション（schema evolution）といいますParquetフォーマットはスキーマエボリューションの機能を有していません。なぜならば一方のシステムで変更を加えた場合、もう一方のシステムにも同時に変更を加えないとならないからです。。 　また、Avroフォーマットのもう一つの大きな特徴は、リッチなスキーマを表現できるという点にあります。キーバリューで表現できるmapやenumも表現可能でフォーマットの機能としてバリデーションも行ってくれます。 　Avroフォーマットにおけるスキーマ定義は以下のような形をしています。この定義に従ってデータ部分をシリアライズしたり、デシリアライズすることでデータを操作していきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KafkaとAvroを連携してみよう\n",
    "1. python(send avro with schema_ver1.avsc) -> kafka <- spark streaming(read avro with schema_ver1.avsc) -> memory\n",
    "2. python(send avro with schema_ver1.avsc) -> kafka <- spark streaming(read avro with schema_ver1.avsc) -> parquet\n",
    "\n",
    "\n",
    "1.の場合は今回はスキーマファイル(schema_ver1.avsc)を使ってシリアライズを実行しkafkaに転送します。  \n",
    "kafkaに到着したデータはSpark Streamingによって弟子リアライズされよみだされ、コンソールに出力されます  \n",
    "\n",
    "2.の場合は最後の出力がparquetに変換され出力されています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# スキーマファイルの確認(schema_ver1.avsc)\n",
    "\n",
    "まず今回のレクチャーで利用するAvroのシリアライズ/デシリアライズ用のスキーマです。  \n",
    "送信したデータ(name/action/sendtime)がkafkaのvalueへ格納されることになります。\n",
    "\n",
    "```\n",
    "{\n",
    "  \"namespace\": \"root\",\n",
    "  \"type\": \"record\",\n",
    "  \"name\": \"Device\",\n",
    "  \"fields\": [\n",
    "    { \"name\": \"id\", \"type\": \"string\" },\n",
    "    { \"name\": \"type\", \"type\": \"string\" },\n",
    "    { \"name\": \"sendtime\", \"type\": \"int\" }\n",
    "  ]\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "Avroでは、スキーマは、4つの要素で成り立っています。\n",
    "\n",
    "name − The value of this field holds the name of the record.\n",
    "レコードの名前(今回だとDeviceに関するレコードですよ。)\n",
    "\n",
    "namespace − The value of this field holds the name of the namespace where the object is stored.\n",
    "データの名前空間\n",
    "\n",
    "type − The value of this attribute holds either the type of the document (record) or the datatype of the field in the schema.\n",
    "ほぼrecordだと思います。\n",
    "\n",
    "fields − This field holds a JSON array, which have the list of all of the fields in the schema, each having name and the type attributes.\n",
    "カラムの定義を行う部分です"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コンソールで設定したSparkとNoteBookを接続します(動かす前に毎度実行する必要があります)\n",
    "import findspark\n",
    "findspark.init(\"/home/pyspark/spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pysparkに必要なライブラリを読み込む\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#spark sessionの作成\n",
    "# spark.ui.enabled trueとするとSparkのGUI画面を確認することができます\n",
    "# spark.eventLog.enabled true　とすると　GUIで実行ログを確認することができます\n",
    "# GUIなどの確認は最後のセクションで説明を行います。\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"chapter1\") \\\n",
    "    .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
    "    .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"JST\") \\\n",
    "    .config(\"spark.ui.enabled\",\"true\") \\\n",
    "    .config(\"spark.eventLog.enabled\",\"true\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-streaming_2.13:3.2.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0,org.apache.spark:spark-avro_2.12:3.2.0\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# パッケージを複数渡したい時は「,」で繋いで渡します。\n",
    "# Sparkのバージョンにしっかりと合わせます(今回はSparkのバージョンが3.2を使っています。)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer\n",
    "import avro.schema\n",
    "import avro.io\n",
    "import io\n",
    "import random\n",
    "\n",
    "# スキーマファイルを読み込んでおきます\n",
    "avro_json_schema = open(\"/home/pyspark/pyspark_streaming/schema/schema_ver1.avsc\", \"r\").read()\n",
    "avro_json_schema_ver2 = open(\"/home/pyspark/pyspark_streaming/schema/schema_ver2.avsc\", \"r\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySparkをKafkaと接続します(kafka <- spark streaming(read avro))\n",
    "# ストリームの経路を作成行います。\n",
    "\n",
    "# kafkaからデータを読み取る設定を行います。\n",
    "\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "  .option(\"subscribe\", \"pyspark-topic1\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark streaming(read avro) -> console\n",
    "\n",
    "console_stream_check = df \\\n",
    "  .select(from_avro(\"value\", avro_json_schema).alias(\"json_col\")) \\\n",
    "  .writeStream \\\n",
    "  .trigger(processingTime=\"5 seconds\") \\\n",
    "  .format(\"console\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kafka/console_check/\") \\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark streaming(read avro) -> parquet\n",
    "from pyspark.sql.avro.functions import from_avro, to_avro\n",
    "\n",
    "kafka_parquet = df \\\n",
    "  .select(from_avro(\"value\", avro_json_schema).alias(\"json_col\")) \\\n",
    "  .select(\"json_col.*\") \\\n",
    "  .writeStream \\\n",
    "  .format(\"parquet\") \\\n",
    "  .option(\"path\", \"/tmp/avro_parquet/\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .trigger(processingTime=\"5 seconds\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kafka/avro_parquet/\") \\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window, col\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('id', StringType(), True),\n",
    "    StructField('type', StringType(), True),\n",
    "    StructField('sendtime', StringType(), True),\n",
    "])\n",
    "\n",
    "from pyspark.sql.avro.functions import from_avro, to_avro\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# spark streaming(read avro) -> avro\n",
    "kafka_avro = df.repartition(1).select(\"value\").alias('Device') \\\n",
    "  .writeStream \\\n",
    "  .format(\"avro\") \\\n",
    "  .option(\"path\", \"/tmp/avro_file/\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .trigger(processingTime=\"5 seconds\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kafka/avro_kafka/\") \\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = df \\\n",
    "  .select(from_avro(\"value\", avro_json_schema).alias(\"json_col\")) \\\n",
    "  .writeStream \\\n",
    "  .format(\"memory\") \\\n",
    "  .queryName(\"aggregates8\") \\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_avro.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 送信するpyspark \n",
    "\n",
    "conf = {'bootstrap.servers': 'kafka:9092'}\n",
    "producer = Producer(**conf)\n",
    "\n",
    "# Kafka topic\n",
    "topic = \"pyspark-topic1\"\n",
    "\n",
    "# Path to user.avsc avro schema\n",
    "schema_path = \"/home/pyspark/pyspark_streaming/schema/schema_ver1.avsc\"\n",
    "schema = avro.schema.parse(open(schema_path).read())\n",
    "\n",
    "for i in range(1):\n",
    "    writer = avro.io.DatumWriter(schema)\n",
    "    bytes_writer = io.BytesIO()\n",
    "    encoder = avro.io.BinaryEncoder(bytes_writer)\n",
    "    # データの送信\n",
    "    writer.write({\"id\": \"yuki\",\n",
    "                    \"type\": \"login2\",\n",
    "                    \"sendtime\": \"time\"}, encoder)\n",
    "    raw_bytes = bytes_writer.getvalue()\n",
    "    producer.produce(topic, raw_bytes)\n",
    "producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select json_col.id from aggregates8\").show()\n",
    "spark.sql(\"select json_col.sendtime from aggregates8\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console_stream_check.stop()\n",
    "kafka_parquet.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ファイルがスモール？\n",
    "ファイルが多すぎる場合は、以下の2点を調整することによって調節が可能です。\n",
    "\n",
    "- プロセシング時間の延長\n",
    "- repartitonの付与\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avroファイルの読み書き\n",
    "今回はストリーミングですが、一応Sparkでの読み込みも少しだけおさらいしておきましょう\n",
    "\n",
    "Avroで出力するビッグデータとしての別のメリットはスプリッタブルである（データを分割して複数のサーバーに分けて処理を行うことが可能）という点です。\n",
    "\n",
    "Avroに似ているフォーマットとしてJsonがありますが、こちらはスプリッタブルではなく（厳密には圧縮形式によります）データを分割して複数サーバーで処理するということに不向きです。\n",
    "\n",
    "そのため「Jsonでいいや〜」と思わずに是非Avroでの実装を考えてみてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avro でのデータの読み込み\n",
    "avro_df = spark.read.format(\"avro\").option('avroSchema', avro_json_schema).load(\"/tmp/avro_file8/\")\n",
    "avro_df.printSchema()\n",
    "avro_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avroでのデータ書き込み\n",
    "avro_df.select(\"name\").write.format(\"avro\").save(\"/tmp/avro_etl/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -al /tmp/avro_etl/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet(\"/tmp/avro_parquet/\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avroで後方互換をやってみよう\n",
    "Avroの特徴の一つである後方互換をやってみましょう。\n",
    "\n",
    "今回の後方互換のシナリオとしては、以下を想像してみてください。  \n",
    "各家庭に冷蔵庫が配置されている(IoT機器、パブリッシャー)。  \n",
    "そのIoT機器からは絶え間なくデータが流れている(ストリーミング)  \n",
    "ストリーミングデータの受け口はKafkaを利用し、Spark Streamingでデータを読み込んでいる(コンシューマー)。\n",
    "\n",
    "今回、IoTのソフトウェアがアップデートし温度(temp)も取得することが可能になったのでスキーマのアップデートを行いたい。  \n",
    "しかし、各家庭に配置されている冷蔵庫のソフトウェアのアップデートを行うことは不可能である。\n",
    "\n",
    "こんな時に活躍するのがAvroの後方互換です。\n",
    "\n",
    "今回の後方互換はコンシューマー(Spark Streaming)が古いスキーマバージョンでシリアライズされたデータを新しいスキーマバージョンで読み取ることができるようにしたものです。  \n",
    "この機能を使うことによって、システム側は早々にスキーマバージョンをアップデートし、ゆっくりと各家庭に配置されたソフトウェアが更新されるのを待てば良いことになります。\n",
    "\n",
    "後方互換を行うために、shcema_ver2.avscというファイルを利用します。\n",
    "\n",
    "```\n",
    "{\n",
    "  \"namespace\": \"root\",\n",
    "  \"type\": \"record\",\n",
    "  \"name\": \"Device\",\n",
    "  \"fields\": [\n",
    "    { \"name\": \"id\", \"type\": \"string\" },\n",
    "    { \"name\": \"type\", \"type\": \"string\" },\n",
    "    { \"name\": \"sendtime\", \"type\": \"int\" },\n",
    "    { \"name\": \"temp\", \"type\": \"string\", \"default\": \"1\" }\n",
    "  ]\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "古いスキーマバージョンからの送信であれば1にするという命令を後方互換として設定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to user.avsc avro schema\n",
    "schema_path_ver1 = \"/home/pyspark/pyspark_streaming/schema/schema_ver1.avsc\"\n",
    "schema_ver1 = avro.schema.parse(open(schema_path_ver1).read())\n",
    "\n",
    "\n",
    "schema_path_ver2 = \"/home/pyspark/pyspark_streaming/schema/schema_ver2.avsc\"\n",
    "schema_ver2 = avro.schema.parse(open(schema_path_ver2).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 送信するpyspark \n",
    "# ただし古いバージョンで送ります\n",
    "\n",
    "conf = {'bootstrap.servers': 'kafka:9092'}\n",
    "producer = Producer(**conf)\n",
    "\n",
    "# Kafka topic\n",
    "topic = \"pyspark-topic1\"\n",
    "\n",
    "# Path to user.avsc avro schema\n",
    "schema_path = \"/home/pyspark/pyspark_streaming/schema/schema_ver1.avsc\"\n",
    "schema = avro.schema.parse(open(schema_path).read())\n",
    "\n",
    "for i in range(1):\n",
    "    writer = avro.io.DatumWriter(schema)\n",
    "    bytes_writer = io.BytesIO()\n",
    "    encoder = avro.io.BinaryEncoder(bytes_writer)\n",
    "    # データの送信\n",
    "    writer.write({\"id\": \"yuki_schemaver1\",\n",
    "                    \"type\": \"login2\",\n",
    "                    \"sendtime\": random.randint(0, 10)}, encoder)\n",
    "    raw_bytes = bytes_writer.getvalue()\n",
    "    producer.produce(topic, raw_bytes)\n",
    "producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 送信するpyspark \n",
    "# スキーマバージョン2で送信します\n",
    "\n",
    "conf = {'bootstrap.servers': 'kafka:9092'}\n",
    "producer = Producer(**conf)\n",
    "\n",
    "# Kafka topic\n",
    "topic = \"pyspark-topic1\"\n",
    "\n",
    "# Path to user.avsc avro schema\n",
    "schema_path_ver2 = \"/home/pyspark/pyspark_streaming/schema/schema_ver2.avsc\"\n",
    "schema_ver2 = avro.schema.parse(open(schema_path_ver2).read())\n",
    "\n",
    "for i in range(1):\n",
    "    writer = avro.io.DatumWriter(schema_ver2)\n",
    "    bytes_writer = io.BytesIO()\n",
    "    encoder = avro.io.BinaryEncoder(bytes_writer)\n",
    "    # データの送信\n",
    "    writer.write({\n",
    "                    \"id\": \"yuki_schemaver2\",\n",
    "                    \"type\": \"login2\",\n",
    "                    \"sendtime\": random.randint(0, 10),\n",
    "                    \"temp\": \"1\"\n",
    "                }, encoder)\n",
    "    raw_bytes = bytes_writer.getvalue()\n",
    "    producer.produce(topic, raw_bytes)\n",
    "producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark streaming(read avro) -> console\n",
    "\n",
    "memory_stream_check = df \\\n",
    "  .select(from_avro(\"value\", avro_json_schema_ver2).alias(\"json_col\")) \\\n",
    "  .writeStream \\\n",
    "  .trigger(processingTime=\"5 seconds\") \\\n",
    "  .format(\"memory\") \\\n",
    "  .queryName(\"check_kafka8\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kafka/backword_check/\") \\\n",
    "  .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from check_kafka8\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_parquet.stop()\n",
    "memory_stream_check.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "578f5f657c2fb65ecadb997ad60e5cf2da380ecec34305a6dd913dc5b96e257c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('3.9.1': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
