{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本セクションの目次\n",
    "1. Avroフォーマット\n",
    "2. 前方互換と後方互換と完全互換\n",
    "3. メッセージキューとAvroを連携してみよう\n",
    "4. Avroファイルの読み書き\n",
    "5. Avroで前方互換をやってみよう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kafka / Spark Streaming / Avro\n",
    "kafka / Spark Streaming / Avro はストリーミングシステムを作成する上での3種の神器です。\n",
    "kafka と Spark Stramingについては前セクションで紹介しましたが、今回はそれに加えてAvroフォーマットについて学んでいきましょう\n",
    "\n",
    "## Avro フォーマットとは？\n",
    "\n",
    "もう一つがHadoopの生みの親であるDoug Cutting氏によりプロジェクト化されたAvro（アブロ）フォーマットです【URL】https://avro.apache.org。  \n",
    "Avroフォーマットはおもにストリーミングでのやり取りで効力を発揮するフォーマットです。 \n",
    "\n",
    "元々AvroはHadoopの弱点であったJavaでしか読み書きできないという言語のポータビリティを解決するために生まれました言語の  \n",
    "ポータビリティーが低いということはそのままAvroファイルと連携する対向のシステムの利用言語まで縛ってしまう可能性があります。 　\n",
    "\n",
    "Avroフォーマットの特徴は以下です。 \n",
    "\n",
    "- 行指向フォーマット \n",
    "- 前方互換性と、後方互換性、完全互換を持ち複数のシステム間で速度の違う開発を行うことが可能 \n",
    "- スキーマエボリューションを提供する \n",
    "- Parquetに比べてJSONのようなリッチなフォーマットを表現可能\n",
    "\n",
    "## スキーマファイルを保存する場所\n",
    "スキーマファイルを保存する場所をスキーマレジストリと呼んだりします。  \n",
    "スキーマレジストリとは、共有してファイルを読み取れる場所である必要があります。\n",
    "\n",
    "有名な企業としてConfulentが存在しますが、ただし今回はschemaのディレクトリをスキーマレジストリとします。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avroにおける前方互換、後方互換、完全互換\n",
    "\n",
    "さまざまな言語で利用可能になった今、さらに注目されているAvroの特徴が開発スピードの違いを吸収することができるということです。  \n",
    "一般にデータ基盤が相手をするシステムはスモールデータシステム含め社内のシステムすべてです。  \n",
    "そのシステム群の開発のスピードを合わせようと思ったら組織が大きくなるにつれて調整のコストが増大し調整自体が不可能に近くなります。  \n",
    "\n",
    "そこで、後方互換性や前方互換性というしくみが活躍します。後方互換性とは、新しい製品が、古い製品を扱えることを指します。  \n",
    "\n",
    "前方互換性とは、古い製品が新しい製品を扱えることを指します。  \n",
    "\n",
    "たとえば、Excel2010がExcel2003を扱えるようにすることを後方互換。  \n",
    "Excel2003がExcel2010を扱えるようにすることを前方互換。\n",
    "\n",
    "後方互換や前方互換の機能を利用することによって、一方のシステムへ変更があったときでも、ほかのシステムの稼働を維持しつつ自システムの変更を行うことができるのです。  \n",
    "このようなしくみを提供することをスキーマエボリューション（schema evolution）といいますParquetフォーマットはスキーマエボリューションの機能を有していません。  \n",
    "\n",
    "なぜならば一方のシステムで変更を加えた場合、もう一方のシステムにも同時に変更を加えないとならないからです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KafkaとAvroを連携してみよう\n",
    "\n",
    "今回の連携のイメージを以下に示します。\n",
    "1. python(send avro with schema_ver1.avsc) -> kafka <- spark streaming(read avro with schema_ver1.avsc) -> memory\n",
    "2. python(send avro with schema_ver1.avsc) -> kafka <- spark streaming(read avro with schema_ver1.avsc) -> parquet\n",
    "\n",
    "\n",
    "1.の場合は今回はスキーマファイル(schema_ver1.avsc)を使ってシリアライズを実行しkafkaに転送します。  \n",
    "kafkaに到着したデータはSpark Streamingによってデシリアライズされよみだされ、コンソールに出力されます。  \n",
    "\n",
    "2.の場合は最後の出力がparquetに変換され出力されています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# スキーマファイルの確認(schema_ver1.avsc)\n",
    "\n",
    "まず今回のレクチャーで利用するAvroのシリアライズ/デシリアライズ用のスキーマです。  \n",
    "送信したデータ(name/action/sendtime)がkafkaのvalueへ格納されることになります。\n",
    "\n",
    "```\n",
    "{\n",
    "  \"namespace\": \"root\",\n",
    "  \"type\": \"record\",\n",
    "  \"name\": \"Device\",\n",
    "  \"fields\": [\n",
    "    { \"name\": \"id\", \"type\": \"string\" },\n",
    "    { \"name\": \"type\", \"type\": \"string\" },\n",
    "    { \"name\": \"sendtime\", \"type\": \"int\" }\n",
    "  ]\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "Avroでは、スキーマは、4つの要素で成り立っています。\n",
    "\n",
    "name − The value of this field holds the name of the record.\n",
    "レコードの名前(今回だとDeviceに関するレコードですよ。)\n",
    "\n",
    "namespace − The value of this field holds the name of the namespace where the object is stored.\n",
    "データの名前空間\n",
    "\n",
    "type − The value of this attribute holds either the type of the document (record) or the datatype of the field in the schema.\n",
    "ほぼrecordだと思います。\n",
    "\n",
    "fields − This field holds a JSON array, which have the list of all of the fields in the schema, each having name and the type attributes.\n",
    "カラムの定義を行う部分です"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コンソールで設定したSparkとNoteBookを接続します(動かす前に毎度実行する必要があります)\n",
    "import findspark\n",
    "findspark.init(\"/home/pyspark/spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pysparkに必要なライブラリを読み込む\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#spark sessionの作成\n",
    "# spark.ui.enabled trueとするとSparkのGUI画面を確認することができます\n",
    "# spark.eventLog.enabled true　とすると　GUIで実行ログを確認することができます\n",
    "# GUIなどの確認は最後のセクションで説明を行います。\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"chapter1\") \\\n",
    "    .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
    "    .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"JST\") \\\n",
    "    .config(\"spark.ui.enabled\",\"true\") \\\n",
    "    .config(\"spark.eventLog.enabled\",\"true\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-streaming_2.13:3.2.3,org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.3,org.apache.spark:spark-avro_2.12:3.2.3\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# パッケージを複数渡したい時は「,」で繋いで渡します。\n",
    "# Sparkのバージョンにしっかりと合わせます(今回はSparkのバージョンが3.2.2を使っています。)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer\n",
    "import avro.schema\n",
    "import avro.io\n",
    "import io\n",
    "import random\n",
    "\n",
    "# スキーマファイルを読み込んでおきます\n",
    "avro_json_schema = open(\"/home/pyspark/pyspark_streaming/schema/schema_ver1.avsc\", \"r\").read()\n",
    "avro_json_schema_ver2 = open(\"/home/pyspark/pyspark_streaming/schema/schema_ver2.avsc\", \"r\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySparkをKafkaと接続します(kafka <- spark streaming(read avro))\n",
    "# ストリームの経路を作成行います。\n",
    "\n",
    "# kafkaからデータを読み取る設定を行います。\n",
    "\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "  .option(\"subscribe\", \"pyspark-topic1\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.avro.functions import from_avro, to_avro\n",
    "\n",
    "# spark streaming(read avro) -> console\n",
    "# コンソールのシンクも起動します\n",
    "console_stream_check = df \\\n",
    "  .select(from_avro(\"value\", avro_json_schema).alias(\"json_col\")) \\\n",
    "  .writeStream \\\n",
    "  .trigger(processingTime=\"5 seconds\") \\\n",
    "  .format(\"console\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kafka/console_check/\") \\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファイルのシンクも起動します\n",
    "# spark streaming(read avro) -> parquet\n",
    "from pyspark.sql.avro.functions import from_avro, to_avro\n",
    "\n",
    "kafka_parquet = df \\\n",
    "  .select(from_avro(\"value\", avro_json_schema).alias(\"json_col\")) \\\n",
    "  .select(\"json_col.*\") \\\n",
    "  .writeStream \\\n",
    "  .format(\"parquet\") \\\n",
    "  .option(\"path\", \"/tmp/avro_parquet/\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .trigger(processingTime=\"5 seconds\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kafka/avro_parquet/\") \\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# メモリのシンクも起動します\n",
    "avro_memory = df \\\n",
    "  .select(from_avro(\"value\", avro_json_schema).alias(\"json_col\")) \\\n",
    "  .writeStream \\\n",
    "  .format(\"memory\") \\\n",
    "  .queryName(\"avro_sink\") \\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 送信するpyspark \n",
    "\n",
    "conf = {'bootstrap.servers': 'kafka:9092'}\n",
    "producer = Producer(**conf)\n",
    "\n",
    "# Kafka topic\n",
    "topic = \"pyspark-topic1\"\n",
    "\n",
    "# Path to user.avsc avro schema\n",
    "schema_path = \"/home/pyspark/pyspark_streaming/schema/schema_ver1.avsc\"\n",
    "schema = avro.schema.parse(open(schema_path).read())\n",
    "\n",
    "for i in range(1):\n",
    "    writer = avro.io.DatumWriter(schema)\n",
    "    bytes_writer = io.BytesIO()\n",
    "    encoder = avro.io.BinaryEncoder(bytes_writer)\n",
    "    # データの送信\n",
    "    writer.write({\"id\": \"yuki\",\n",
    "                    \"type\": \"login2\",\n",
    "                    \"sendtime\": random.randint(0, 10)}, encoder)\n",
    "    raw_bytes = bytes_writer.getvalue()\n",
    "    producer.produce(topic, raw_bytes)\n",
    "producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select json_col.id from avro_sink\").show()\n",
    "spark.sql(\"select json_col.type,json_col.sendtime from avro_sink\").show()\n",
    "\n",
    "#のちのために変数に入れておきます\n",
    "avro_file=spark.sql(\"select * from avro_sink\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console_stream_check.stop()\n",
    "kafka_parquet.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ファイルがスモール？\n",
    "ファイルが多すぎる場合は、以下の2点を調整することによって調節が可能です。\n",
    "\n",
    "- プロセシング時間の延長\n",
    "- repartitonの付与\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avroファイルの読み書き\n",
    "今回はストリーミングですが、一応Sparkでの読み込みも少しだけおさらいしておきましょう\n",
    "\n",
    "Avroで出力するビッグデータとしての別のメリットはスプリッタブルである（データを分割して複数のサーバーに分けて処理を行うことが可能）という点です。\n",
    "\n",
    "Avroに似ているフォーマットとしてJsonがありますが、こちらはスプリッタブルではなく（厳密には圧縮形式によります）データを分割して複数サーバーで処理するということに不向きです。\n",
    "\n",
    "そのため「Jsonでいいや〜」と思わずに是非Avroでの実装を考えてみてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avroでのデータ書き込み\n",
    "avro_file.write.mode('overwrite').format(\"avro\").save(\"/tmp/avro_etl/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avro でのデータの読み込み\n",
    "avro_df = spark.read.format(\"avro\").load(\"/tmp/avro_etl/\")\n",
    "avro_df.printSchema()\n",
    "avro_df.show()\n",
    "avro_df.select(\"json_col.*\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -al /tmp/avro_etl/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ちなみにParquetのシンクもみてみましょう\n",
    "spark.read.parquet(\"/tmp/avro_parquet/\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avroで前方互換をやってみよう\n",
    "Avroの特徴の一つである前方互換をやってみましょう。\n",
    "\n",
    "今回の前方互換のシナリオとしては、以下を想像してみてください。  \n",
    "各家庭に冷蔵庫が配置されている(IoT機器、パブリッシャー)。  \n",
    "そのIoT機器からは絶え間なくデータが流れている(ストリーミング)  \n",
    "ストリーミングデータの受け口はKafkaを利用し、Spark Streamingでデータを読み込んでいる(コンシューマー)。\n",
    "\n",
    "今回、IoTのソフトウェアがアップデートし温度(temp)も取得することが可能になったのでスキーマのアップデートを行いたい。  \n",
    "しかし、各家庭に配置されている冷蔵庫のソフトウェアのアップデートを行うことは不可能である。\n",
    "\n",
    "今回の前方互換はコンシューマー(Spark Streaming)が新しいスキーマバージョンでシリアライズされたデータを古いスキーマバージョンで読み取ることができるようにしたものです。  \n",
    "この機能を使うことによって、システム側はスキーマバージョンのアップデートを先延ばしにして、ゆっくりと各家庭に配置されたソフトウェアが更新されるのを待てば良いことになります。\n",
    "\n",
    "前方互換を行うために、shcema_ver2.avscというファイルを利用します。\n",
    "\n",
    "```\n",
    "{\n",
    "  \"namespace\": \"root\",\n",
    "  \"type\": \"record\",\n",
    "  \"name\": \"Device\",\n",
    "  \"fields\": [\n",
    "    { \"name\": \"id\", \"type\": \"string\" },\n",
    "    { \"name\": \"type\", \"type\": \"string\" },\n",
    "    { \"name\": \"sendtime\", \"type\": \"int\" },\n",
    "    { \"name\": \"temp\", \"type\": \"string\", \"default\": \"1\" }\n",
    "  ]\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to user.avsc avro schema\n",
    "schema_path_ver1 = \"/home/pyspark/pyspark_streaming/schema/schema_ver1.avsc\"\n",
    "schema_ver1 = avro.schema.parse(open(schema_path_ver1).read())\n",
    "\n",
    "\n",
    "schema_path_ver2 = \"/home/pyspark/pyspark_streaming/schema/schema_ver2.avsc\"\n",
    "schema_ver2 = avro.schema.parse(open(schema_path_ver2).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark streaming(read avro) -> memory\n",
    "# スキーマバージョン1でデータ読み取ります\n",
    "from pyspark.sql.avro.functions import from_avro, to_avro\n",
    "\n",
    "memory_stream_check = df \\\n",
    "  .select(df.key,from_avro(\"value\", avro_json_schema, {\"mode\" : \"PERMISSIVE\"}).alias(\"json_col\")) \\\n",
    "  .writeStream \\\n",
    "  .trigger(processingTime=\"5 seconds\") \\\n",
    "  .format(\"memory\") \\\n",
    "  .queryName(\"check_kafka11\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kafka/backword_check11/\") \\\n",
    "  .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 送信するpyspark \n",
    "# ただし古いバージョンで送ります\n",
    "\n",
    "conf = {'bootstrap.servers': 'kafka:9092'}\n",
    "producer = Producer(**conf)\n",
    "\n",
    "# Kafka topic\n",
    "topic = \"pyspark-topic1\"\n",
    "\n",
    "# Path to user.avsc avro schema\n",
    "schema_path = \"/home/pyspark/pyspark_streaming/schema/schema_ver1.avsc\"\n",
    "schema = avro.schema.parse(open(schema_path).read())\n",
    "\n",
    "for i in range(1):\n",
    "    writer = avro.io.DatumWriter(schema)\n",
    "    bytes_writer = io.BytesIO()\n",
    "    encoder = avro.io.BinaryEncoder(bytes_writer)\n",
    "    # データの送信\n",
    "    writer.write({\"id\": \"yuki_schemaver1\",\n",
    "                    \"type\": \"login2\",\n",
    "                    \"sendtime\": random.randint(0, 10)}, encoder)\n",
    "    raw_bytes = bytes_writer.getvalue()\n",
    "    producer.produce(topic, raw_bytes, \"1\")\n",
    "producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 送信するpyspark \n",
    "# スキーマバージョン2で送信します\n",
    "\n",
    "conf = {'bootstrap.servers': 'kafka:9092'}\n",
    "producer_ver2 = Producer(**conf)\n",
    "\n",
    "# Kafka topic\n",
    "topic = \"pyspark-topic1\"\n",
    "\n",
    "# Path to user.avsc avro schema\n",
    "schema_path_ver2 = \"/home/pyspark/pyspark_streaming/schema/schema_ver2.avsc\"\n",
    "schema_ver2 = avro.schema.parse(open(schema_path_ver2).read())\n",
    "\n",
    "for i in range(1):\n",
    "    writer = avro.io.DatumWriter(schema_ver2)\n",
    "    bytes_writer = io.BytesIO()\n",
    "    encoder = avro.io.BinaryEncoder(bytes_writer)\n",
    "    # データの送信\n",
    "    writer.write({\n",
    "                    \"id\": \"ver2_yuki_schema\",\n",
    "                    \"type\": \"login2\",\n",
    "                    \"sendtime\": random.randint(0, 10),\n",
    "                    \"temp\": \"1000\"\n",
    "                }, encoder)\n",
    "    raw_bytes = bytes_writer.getvalue()\n",
    "    producer_ver2.produce(topic, raw_bytes, \"2\")\n",
    "producer_ver2.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#結果\n",
    "spark.sql(\"select cast(key as string),json_col.id from check_kafka11\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_parquet.stop()\n",
    "memory_stream_check.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "578f5f657c2fb65ecadb997ad60e5cf2da380ecec34305a6dd913dc5b96e257c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('3.9.1': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
