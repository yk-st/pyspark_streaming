{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本セクションの目次\n",
    "1. Spark Structured Streamingとは？\n",
    "2. メッセージキューとの組み合わせは？\n",
    "3. 他のストリーミングツール"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Structured Streamingとは？\n",
    "Spark Structured Streamingとは\n",
    "「Spark Streaming」はApache Sparkをストリーミング用途で利用する場合に使われる呼び方です。  \n",
    "ですので、Sparkがあればバッチ処理からストリーミング処理まで完結します。ビッグデータの処理において、使い勝手の良い技術の一つでしょう。  \n",
    "\n",
    "## PySparkのバッチとはどのように違う？\n",
    "あくまでSparkの中の機能の一つなので、基本的な使い方は変わりません。  \n",
    "そのためデータフレームの操作やSparkSQLの操作はバッチと変わりません。\n",
    "\n",
    "しかし、ストリーミング用の関数を使う必要があったりすることと、登場人物が多い(Kafka（メッセージキュー）などとの連携がある)のでプログラムが複雑になりやすいです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kafkaとの組み合わせはどんな所に使われる？\n",
    "Kafkaとの組み合わせでは、Spark Streamingはパブリッシャーになることもできますしコンシューマーになることも可能です。\n",
    "\n",
    "##  kafkaに送信されるデータの形\n",
    "kafkaに送信するデータはキー、バリュー型です。\n",
    "\n",
    "```\n",
    "root\n",
    " |-- key: binary (nullable = true)   \n",
    " |-- value: binary (nullable = true)\n",
    " |-- topic: string (nullable = true)\n",
    " |-- partition: integer (nullable = true)\n",
    " |-- offset: long (nullable = true)\n",
    " |-- timestamp: timestamp (nullable = true)\n",
    " |-- timestampType: integer (nullable = true)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "## コマンドラインがパブリッシャーの例とコンシューマーの例\n",
    "\n",
    "cmd -> kafka <- spark streamingという流れになります。\n",
    "\n",
    "```\n",
    " /home/pyspark/kafka/bin/kafka-console-producer.sh --topic pyspark-topic1 --bootstrap-server kafka:9092 \\\n",
    " --property parse.key=true --property key.separator=,\n",
    "\n",
    "```\n",
    "\n",
    "ーーーーkafkaがデータを保持する\n",
    "\n",
    "\n",
    "```\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "  .option(\"subscribe\", \"pyspark-topic1\") \\\n",
    "  .load()\n",
    "```\n",
    "\n",
    "## Spark Streamingがパブリッシャーの例とコンシューマーの例\n",
    "\n",
    "cmd -> kafka <- spark streaming -> kafkaという流れになります。\n",
    "\n",
    "```\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "  .option(\"subscribe\", \"pyspark-topic1\") \\\n",
    "  .load()\n",
    "```\n",
    "\n",
    "Streamingがkafkaに対して書き込みを行う。別トピックにする。\n",
    "```\n",
    "ds = df \\\n",
    "  .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "  .writeStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "  .option(\"topic\", \"pyspark-topic2\") \\\n",
    "  .option(\"checkpointLocation\",\"/tmp/kafka\") \\\n",
    "  .queryName(\"aggregates\") \\\n",
    "  .start()\n",
    "```\n",
    "\n",
    "ーーーーkafka(topic 2)がデータを保持する\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 他のストリーミングツール\n",
    "Spark Streaming以外でストリーミング処理(プログラム可能なもの)で使われるツールを紹介します。\n",
    "\n",
    "- Apache Beam\n",
    "\n",
    "特に大きな違いはないのですが、GoogleであればGoogle Dataflowというマネージドサービスが出ています。  \n",
    "少し言葉の定義などに癖があるのですが、Sparkを理解していればBeamの理解は簡単なはずです。  \n",
    "\n",
    "Apache BeamはPythonやJava、Goなどに対応しています。"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "578f5f657c2fb65ecadb997ad60e5cf2da380ecec34305a6dd913dc5b96e257c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('3.9.1': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
