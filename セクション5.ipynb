{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本セクションの目次"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# クイックスタートKafkaとPyspark\n",
    "\n",
    "1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kafkaのトピックを作成します\n",
    "今回は3つのトピックを作成します\n",
    "\n",
    "```\n",
    "/home/pyspark/kafka/bin/kafka-topics.sh \\\n",
    "    --create --topic pyspark-topic1 \\\n",
    "    --replication-factor 1 \\\n",
    "    --partitions 1 \\\n",
    "    --bootstrap-server kafka:9092 \n",
    "```\n",
    "\n",
    "```\n",
    "/home/pyspark/kafka/bin/kafka-topics.sh \\\n",
    "    --create --topic pyspark-topic2 \\\n",
    "    --replication-factor 1 \\\n",
    "    --partitions 1 \\\n",
    "    --bootstrap-server kafka:9092 \n",
    "```\n",
    "\n",
    "```\n",
    "/home/pyspark/kafka/bin/kafka-topics.sh \\\n",
    "    --create --topic pyspark-topic3 \\\n",
    "    --replication-factor 1 \\\n",
    "    --partitions 1 \\\n",
    "    --bootstrap-server kafka:9092 \n",
    "```\n",
    "\n",
    "## replication-factor/partitions　\n",
    "いわゆる冗長化の設定。\n",
    "レプリケーションは何台に\n",
    "パーティションは何個？\n",
    "という意味の冗長化。\n",
    "\n",
    "つまりreplication-factor 2でpartitons3であれば2台のノードにまたがってパーティションを3つ（1台に２つ、もう一台に1つ）  \n",
    "という設定になる。\n",
    "\n",
    "## bootstrap-server \n",
    "データの受付口（ブローカーのIPとポートを指定する）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コンソールで設定したSparkとNoteBookを接続します(動かす前に毎度実行する必要があります)\n",
    "import findspark\n",
    "findspark.init(\"/home/pyspark/spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pysparkに必要なライブラリを読み込む\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#spark sessionの作成\n",
    "# spark.ui.enabled trueとするとSparkのGUI画面を確認することができます\n",
    "# spark.eventLog.enabled true　とすると　GUIで実行ログを確認することができます\n",
    "# GUIなどの確認は最後のセクションで説明を行います。\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"chapter1\") \\\n",
    "    .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
    "    .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"JST\") \\\n",
    "    .config(\"spark.ui.enabled\",\"true\") \\\n",
    "    .config(\"spark.eventLog.enabled\",\"true\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-streaming_2.13:3.2.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0,org.apache.spark:spark-avro_2.12:3.2.0\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# パッケージを複数渡したい時は「,」で繋いで渡します。\n",
    "# Sparkのバージョンにしっかりと合わせます(今回はSparkのバージョンが3.2を使っています。)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kafkaにデータを送信してそのデータをコンソールに出してみよう\n",
    "\n",
    "まずは最も単純にkafkaにデータをコマンドラインで送信しそのデータを読み取ってconsole出力してみましょう\n",
    "\n",
    "cmd -> kafka <- spark streaming(df) -> console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySparkをKafkaと接続します(kafka <- spark streaming(df))\n",
    "# ストリームの経路を作成行います。\n",
    "\n",
    "# kafkaからデータを読み取る設定を行います。\n",
    "\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "  .option(\"subscribe\", \"pyspark-topic1\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/2q/ffspqcy527ggw9qyccxdymd40000gn/T/ipykernel_87002/2644214831.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# ストリームから読み取って出力する設定(spark streaming(df) -> console)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mconsole_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0mselectExpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CAST(key AS STRING)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"CAST(value AS STRING)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# ストリームから読み取って出力する設定(spark streaming(df) -> console)\n",
    "\n",
    "console_stream = df \\\n",
    "  .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "  .writeStream \\\n",
    "  .trigger(processingTime=\"5 seconds\") \\\n",
    "  .format(\"console\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kafka/console/\") \\\n",
    "  .start()\n",
    "\n",
    "# ストリームから読み取ったデータのうちkeyとvalueを抜き出してStringにキャストしたのちconsoleに出力してね\n",
    "# triggerは何秒単位で処理を行うか（処理というのは↑のconsoleに出力する処理のこと）今回は動画の都合で短め(おおよそ1~10分が多い)\n",
    "# checkpointlocationは、処理をすると「ここまで処理をしたよ！」という情報が出力される場所のこと\n",
    "# 仮にkafkaが落ちてもチェックポイントのデータが生きていれば復旧した後に途中からやり直しをすることが可能になる。\n",
    "\n",
    "#root\n",
    "# |-- key: binary (nullable = true)   \n",
    "# |-- value: binary (nullable = true)\n",
    "# |-- topic: string (nullable = true)\n",
    "# |-- partition: integer (nullable = true)\n",
    "# |-- offset: long (nullable = true)\n",
    "# |-- timestamp: timestamp (nullable = true)\n",
    "# |-- timestampType: integer (nullable = true)\n",
    "\n",
    "# binary型なのは様々な型のデータが入ってくることが想定されるから"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 送信コマンド(cmd -> kafka)\n",
    "\n",
    "```\n",
    "\n",
    "/home/pyspark/kafka/bin/kafka-console-producer.sh \\\n",
    "   --topic pyspark-topic1  \\\n",
    "   --bootstrap-server kafka:9092 \\\n",
    "   --property parse.key=true --property key.separator=,\n",
    "\n",
    "```\n",
    "\n",
    "key.separatorはキーバリューを分割するためのオプションです、今回はカンマで区切るとキーとバリューを送ることが可能です。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ストリームの停止\n",
    "console_stream.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ファイルシンクしてみよう\n",
    "次は、コンソールではなくファイル内にシンクをしてみましょう\n",
    "\n",
    "cmd -> kafka <- spark streaming(df) -> parquet(ファイル)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ストリームから読み取って出力する設定(spark streaming(df) -> file)\n",
    "\n",
    "file_stream = df \\\n",
    "  .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "  .writeStream \\\n",
    "  .format(\"parquet\") \\\n",
    "  .option(\"path\", \"/tmp/data/\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .partitionBy(\"key\") \\\n",
    "  .trigger(processingTime=\"5 seconds\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kafka/parquet/\") \\\n",
    "  .start()\n",
    "\n",
    "# ほぼフルフルでのオプションです(必要に応じてオプションを付け足したりします)\n",
    "# formatはparqeut\n",
    "# pathはどこに出力するか\n",
    "# 追記か？overwrite(上書きか)\n",
    "# partitionbyパーティションを付ける場合\n",
    "# 読み込みの状況を途中保存するためのチェックポイント\n",
    "\n",
    "# 単純なテキストがparquetに変換されて出力されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_df=spark.read.parquet(\"/tmp/data/*\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "578f5f657c2fb65ecadb997ad60e5cf2da380ecec34305a6dd913dc5b96e257c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('3.9.1': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
