{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本セクションの目次\n",
    "\n",
    "1. コンポーネントの起動\n",
    "2. データの送信と確認\n",
    "3. データの送信と確認(コンソール)\n",
    "4. データの送信と確認(ファイル)\n",
    "4. データの送信と確認(メッセージ)\n",
    "4. データの送信と確認(メモリ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# コンポーネントの起動\n",
    "既に起動はしているのですが、一度関係しているコンポーネントを覗いてみましょう。\n",
    "\n",
    "## kafkaのトピックを作成します\n",
    "今回は3つのトピックを作成します\n",
    "\n",
    "```\n",
    "/home/pyspark/kafka/bin/kafka-topics.sh \\\n",
    "    --create --topic pyspark-topic1 \\\n",
    "    --replication-factor 1 \\\n",
    "    --partitions 1 \\\n",
    "    --bootstrap-server kafka:9092 \n",
    "```\n",
    "\n",
    "```\n",
    "/home/pyspark/kafka/bin/kafka-topics.sh \\\n",
    "    --create --topic pyspark-topic2 \\\n",
    "    --replication-factor 1 \\\n",
    "    --partitions 1 \\\n",
    "    --bootstrap-server kafka:9092 \n",
    "```\n",
    "\n",
    "```\n",
    "/home/pyspark/kafka/bin/kafka-topics.sh \\\n",
    "    --create --topic pyspark-topic5 \\\n",
    "    --replication-factor 1 \\\n",
    "    --partitions 3 \\\n",
    "    --bootstrap-server kafka:9092 \n",
    "```\n",
    "\n",
    "## replication-factor/partitions　\n",
    "いわゆる冗長化の設定。\n",
    "レプリケーションは何台に\n",
    "パーティションは何個？\n",
    "という意味の冗長化。\n",
    "\n",
    "つまりreplication-factor 2でpartitons3であれば2台のノードにまたがってパーティションを3つ（1台に２つ、もう一台に1つ）  \n",
    "という設定になる。\n",
    "\n",
    "## bootstrap-server \n",
    "データの受付口（ブローカーのIPとポートを指定する）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの送信と確認\n",
    "\n",
    "データの送信方法は何種類か存在します。\n",
    "\n",
    "どのような場面でも対応できるように本コースでは以下の3つを対象とします。  \n",
    "プログラムから送信するパターンとWeb画面から送信するパターンはのちのセクションにて別途抜き出して解説します。\n",
    "\n",
    "- コマンドライン\n",
    "- プログラムからデータを送信する\n",
    "- Web画面から送信する\n",
    "\n",
    "またデータの確認方法としてシンクを紹介します\n",
    "\n",
    "- コンソールシンク\n",
    "- メモリシンク\n",
    "- メッセージシンク\n",
    "- ファイルシンク\n",
    "\n",
    "を紹介します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コンソールで設定したSparkとNoteBookを接続します(動かす前に毎度実行する必要があります)\n",
    "import findspark\n",
    "findspark.init(\"/home/pyspark/spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.exec.dynamic.partition.mode\n",
      "Warning: Ignoring non-Spark config property: hive.exec.dynamic.partition\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/pyspark/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/pyspark/spark-3.2.0-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/pyspark/.ivy2/cache\n",
      "The jars for the packages stored in: /home/pyspark/.ivy2/jars\n",
      "org.apache.spark#spark-streaming_2.13 added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.spark#spark-avro_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f35659d7-ccbc-409d-b0bb-cddb6f0ec4a0;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.0 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.1 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.1 in central\n",
      "\tfound org.apache.htrace#htrace-core4;4.1.0-incubating in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      "\tfound org.apache.spark#spark-avro_2.12;3.2.0 in central\n",
      "\tfound org.tukaani#xz;1.8 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.2.0/spark-sql-kafka-0-10_2.12-3.2.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0!spark-sql-kafka-0-10_2.12.jar (392ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.12/3.2.0/spark-avro_2.12-3.2.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-avro_2.12;3.2.0!spark-avro_2.12.jar (170ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.2.0/spark-token-provider-kafka-0-10_2.12-3.2.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0!spark-token-provider-kafka-0-10_2.12.jar (136ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/2.8.0/kafka-clients-2.8.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;2.8.0!kafka-clients.jar (668ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (137ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.6.2/commons-pool2-2.6.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-pool2;2.6.2!commons-pool2.jar (140ms)\n",
      "downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...\n",
      "\t[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (126ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.1/hadoop-client-runtime-3.3.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.3.1!hadoop-client-runtime.jar (3785ms)\n",
      "downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.7.1/lz4-java-1.7.1.jar ...\n",
      "\t[SUCCESSFUL ] org.lz4#lz4-java;1.7.1!lz4-java.jar (226ms)\n",
      "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.8.4/snappy-java-1.1.8.4.jar ...\n",
      "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.8.4!snappy-java.jar(bundle) (301ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.30!slf4j-api.jar (144ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.1/hadoop-client-api-3.3.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.1!hadoop-client-api.jar (2218ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/htrace/htrace-core4/4.1.0-incubating/htrace-core4-4.1.0-incubating.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.htrace#htrace-core4;4.1.0-incubating!htrace-core4.jar (613ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...\n",
      "\t[SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (139ms)\n",
      "downloading https://repo1.maven.org/maven2/org/tukaani/xz/1.8/xz-1.8.jar ...\n",
      "\t[SUCCESSFUL ] org.tukaani#xz;1.8!xz.jar (141ms)\n",
      ":: resolution report :: resolve 24693ms :: artifacts dl 9349ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.1 from central in [default]\n",
      "\torg.apache.htrace#htrace-core4;4.1.0-incubating from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.0 from central in [default]\n",
      "\torg.apache.spark#spark-avro_2.12;3.2.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.tukaani#xz;1.8 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   15  |   15  |   15  |   0   ||   15  |   15  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f35659d7-ccbc-409d-b0bb-cddb6f0ec4a0\n",
      "\tconfs: [default]\n",
      "\t15 artifacts copied, 0 already retrieved (59469kB/68ms)\n",
      "21/12/12 01:53:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "#pysparkに必要なライブラリを読み込む\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#spark sessionの作成\n",
    "# spark.ui.enabled trueとするとSparkのGUI画面を確認することができます\n",
    "# spark.eventLog.enabled true　とすると　GUIで実行ログを確認することができます\n",
    "# GUIなどの確認は最後のセクションで説明を行います。\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"chapter1\") \\\n",
    "    .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
    "    .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"JST\") \\\n",
    "    .config(\"spark.ui.enabled\",\"true\") \\\n",
    "    .config(\"spark.eventLog.enabled\",\"true\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-streaming_2.13:3.2.2,org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.2,org.apache.spark:spark-avro_2.12:3.2.2\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# パッケージを複数渡したい時は「,」で繋いで渡します。\n",
    "# Sparkのバージョンにしっかりと合わせます(今回はSparkのバージョンが3.2.2を使っています。)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kafkaにデータを送信してそのデータをコンソールに出してみよう\n",
    "\n",
    "まずは最も単純にkafkaにデータをコマンドラインで送信しそのデータを読み取ってconsole出力してみましょう\n",
    "\n",
    "cmd -> kafka <- spark streaming(df) -> console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySparkをKafkaと接続します(kafka <- spark streaming(df))\n",
    "# ストリームの経路を作成行います。\n",
    "\n",
    "# kafkaからデータを読み取る設定を行います。\n",
    "\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "  .option(\"subscribe\", \"pyspark-topic1\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/12 01:58:37 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---+-----+\n",
      "|key|value|\n",
      "+---+-----+\n",
      "+---+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+---+-----+\n",
      "|key|value|\n",
      "+---+-----+\n",
      "|  1|    2|\n",
      "|  2|    1|\n",
      "+---+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+---+-----+\n",
      "|key|value|\n",
      "+---+-----+\n",
      "|  3|    1|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ストリームから読み取って出力する設定(spark streaming(df) -> console)\n",
    "\n",
    "console_stream = df \\\n",
    "  .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "  .writeStream \\\n",
    "  .trigger(processingTime=\"5 seconds\") \\\n",
    "  .format(\"console\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kafka/console/\") \\\n",
    "  .start()\n",
    "\n",
    "# ストリームから読み取ったデータのうちkeyとvalueを抜き出してStringにキャストしたのちconsoleに出力してね\n",
    "# triggerは何秒単位で処理を行うか（処理というのは↑のconsoleに出力する処理のこと）今回は動画の都合で短め(おおよそ1~10分が多い)\n",
    "# checkpointlocationは、処理をすると「ここまで処理をしたよ！」という情報が出力される場所のこと\n",
    "# 仮にkafkaが落ちてもチェックポイントのデータが生きていれば復旧した後に途中からやり直しをすることが可能になる。\n",
    "\n",
    "#root\n",
    "# |-- key: binary (nullable = true)   \n",
    "# |-- value: binary (nullable = true)\n",
    "# |-- topic: string (nullable = true)\n",
    "# |-- partition: integer (nullable = true)\n",
    "# |-- offset: long (nullable = true)\n",
    "# |-- timestamp: timestamp (nullable = true)\n",
    "# |-- timestampType: integer (nullable = true)\n",
    "\n",
    "# binary型なのは様々な型のデータが入ってくることが想定されるから"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 送信コマンド(cmd -> kafka)\n",
    "\n",
    "```\n",
    "\n",
    "/home/pyspark/kafka/bin/kafka-console-producer.sh \\\n",
    "   --topic pyspark-topic1  \\\n",
    "   --bootstrap-server kafka:9092 \\\n",
    "   --property parse.key=true --property key.separator=,\n",
    "\n",
    "```\n",
    "\n",
    "key.separatorはキーバリューを分割するためのオプションです、今回はカンマで区切るとキーとバリューを送ることが可能です。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ストリームの停止\n",
    "console_stream.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ファイルシンクしてみよう\n",
    "次は、コンソールではなくファイル内にシンクをしてみましょう\n",
    "\n",
    "cmd -> kafka <- spark streaming(df) -> parquet(ファイル)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/12 02:02:16 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "# ストリームから読み取って出力する設定(spark streaming(df) -> file)\n",
    "\n",
    "file_stream = df \\\n",
    "  .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "  .writeStream \\\n",
    "  .format(\"parquet\") \\\n",
    "  .option(\"path\", \"/tmp/data/\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .partitionBy(\"key\") \\\n",
    "  .trigger(processingTime=\"5 seconds\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kafka/parquet/\") \\\n",
    "  .start()\n",
    "\n",
    "# ほぼフルフルでのオプションです(必要に応じてオプションを付け足したりします)\n",
    "# formatはparqeut\n",
    "# pathはどこに出力するか\n",
    "# append(追記か？)overwrite(上書きか)\n",
    "# partitionbyパーティションを付ける場合\n",
    "# 読み込みの状況を途中保存するためのチェックポイント\n",
    "\n",
    "# 単純なテキストがparquetに変換されて出力されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_stream.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|    1|\n",
      "|    1|\n",
      "|    6|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "read_df=spark.read.parquet(\"/tmp/data/*/*\")\n",
    "read_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# メッセージシンクをしてみよう\n",
    "メッセージシンクとはメッセージキューから読み取ったデータを別のメッセージキューに送信することです  \n",
    "cmd -> kafka(pyspark-topic1) <- spark streaming(df) -> kafka(pyspark-topic2)  <- spark streaming(df2)(データの確認のため) -> console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データのチェックを行うためのストリームを生成します\n",
    "\n",
    "# PySparkをKafkaと接続します(kafka(pyspark-topic2)  <- spark streaming(df2)(データの確認のため))\n",
    "# ストリームの経路を作成行います。\n",
    "\n",
    "# kafkaからデータを読み取る設定を行います。\n",
    "\n",
    "df2 = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "  .option(\"subscribe\", \"pyspark-topic2\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/12 02:04:58 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---+-----+\n",
      "|key|value|\n",
      "+---+-----+\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PySparkをKafkaと接続します(<- spark streaming(df2)(データの確認のため) -> console)\n",
    "# ストリームの経路を作成行います。\n",
    "\n",
    "# kafkaからデータを読み取る設定を行います。\n",
    "\n",
    "console_stream_check = df2 \\\n",
    "  .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "  .writeStream \\\n",
    "  .trigger(processingTime=\"5 seconds\") \\\n",
    "  .format(\"console\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kafka/console_check/\") \\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/12 02:06:41 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+---+-----+\n",
      "|key|value|\n",
      "+---+-----+\n",
      "| 34|   11|\n",
      "+---+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+---+-----+\n",
      "|key|value|\n",
      "+---+-----+\n",
      "| 22|   11|\n",
      "| 11|   23|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ストリームから読み取ってメッセージシンクする設定(spark streaming(df) -> kafka(pyspark-tpoic2))\n",
    "\n",
    "kafka_stream = df \\\n",
    "  .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "  .writeStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "  .option(\"topic\", \"pyspark-topic2\") \\\n",
    "  .trigger(processingTime=\"5 seconds\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kafka/pyspark-topic2/\") \\\n",
    "  .start()\n",
    "\n",
    "# ほぼフルフルでのオプションです(必要に応じてオプションを付け足したりします)\n",
    "# formatはkafka\n",
    "# ブローカーのサーバのIPを指定\n",
    "# どのトピックに対して書き込みを行うか記載\n",
    "# 読み込みの状況を途中保存するためのチェックポイント"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "console_stream_check.stop()\n",
    "kafka_stream.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# メモリシンクをしてみよう\n",
    "メモリシンクとはその名の通りデータをメモリ内に保持することです  \n",
    "cmd -> kafka(pyspark-topic1) <- spark streaming(df) -> メモリ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/12 02:09:52 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "memory_sink = df \\\n",
    "  .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "  .writeStream \\\n",
    "  .format(\"memory\") \\\n",
    "  .queryName(\"aggregates\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kafka/memory/\") \\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "|key|    value|\n",
      "+---+---------+\n",
      "| 11|111111111|\n",
      "|  2|        2|\n",
      "|222|      111|\n",
      "|333|     333,|\n",
      "| aa|     1111|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from aggregates\").show()\n",
    "\n",
    "# メモリシンクについてはまだ使い道があるので、のちのセクションで改めて紹介します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/12 02:11:48 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-72b67146-1e6d-4791-ac8e-3487aff5b6ea. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "21/12/12 02:11:48 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "#　同時にサブスクライブもできる\n",
    "memory_sink2 = df \\\n",
    "  .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "  .writeStream \\\n",
    "  .format(\"memory\") \\\n",
    "  .queryName(\"aggregates2\") \\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|key|value|\n",
      "+---+-----+\n",
      "| aa| 1111|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from aggregates2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_sink.stop()\n",
    "memory_sink2.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 各シンクの使い分け\n",
    "\n",
    "1. コンソールシンク  \n",
    "コンソールシンクは、デバッグ用として主に利用することが多いです\n",
    "2. ファイルシンク  \n",
    "一度ストレージ(S3やHDFS)に保存して後でバッチ処理を行う\n",
    "3. kafkaシンク  \n",
    "別のシステムとの連携や複雑な計算や変換を行った後の中間データとして配置する\n",
    "4. メモリシンク  \n",
    "リアルタイムでデータを集計したりするために使う\n",
    "\n",
    "今回データはcmdから送信していましたが、次のセクションは一歩進んでPythonからデータを送信しつつAvro形式でのやり取りをみていこうと思います。"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "578f5f657c2fb65ecadb997ad60e5cf2da380ecec34305a6dd913dc5b96e257c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('3.9.1': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
