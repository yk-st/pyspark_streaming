{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kafkaのトピック(pyspark-topic10)を作成してみましょう\n",
    "\n",
    "```\n",
    "/home/pyspark/kafka/bin/kafka-topics.sh \\\n",
    "    --create --topic pyspark-topic10 \\\n",
    "    --replication-factor 1 \\\n",
    "    --partitions 1 \\\n",
    "    --bootstrap-server kafka:9092 \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 後方互換\n",
    "古いスキーマバージョンでシリアライズされたデータを新しいスキーマバージョンでコンシュームすることが可能にすることです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コンソールで設定したSparkとNoteBookを接続します(動かす前に毎度実行する必要があります)\n",
    "import findspark\n",
    "findspark.init(\"/home/pyspark/spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pysparkに必要なライブラリを読み込む\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#spark sessionの作成\n",
    "# spark.ui.enabled trueとするとSparkのGUI画面を確認することができます\n",
    "# spark.eventLog.enabled true　とすると　GUIで実行ログを確認することができます\n",
    "# GUIなどの確認は最後のセクションで説明を行います。\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"chapter1\") \\\n",
    "    .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
    "    .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"JST\") \\\n",
    "    .config(\"spark.ui.enabled\",\"true\") \\\n",
    "    .config(\"spark.eventLog.enabled\",\"true\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-streaming_2.13:3.2.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0,org.apache.spark:spark-avro_2.12:3.2.0\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# パッケージを複数渡したい時は「,」で繋いで渡します。\n",
    "# Sparkのバージョンにしっかりと合わせます(今回はSparkのバージョンが3.2を使っています。)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "  .option(\"subscribe\", \"pyspark-topic10\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer\n",
    "import avro.schema\n",
    "import avro.io\n",
    "import io\n",
    "import random\n",
    "\n",
    "# スキーマファイルを読み込んでおきます\n",
    "avro_json_schema = open(\"/home/pyspark/pyspark_streaming/schema/schema_ver1.avsc\", \"r\").read()\n",
    "avro_json_schema_ver2 = open(\"/home/pyspark/pyspark_streaming/schema/schema_ver2.avsc\", \"r\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 送信するpyspark \n",
    "# ただし古いバージョンで送ります\n",
    "\n",
    "conf = {'bootstrap.servers': 'kafka:9092'}\n",
    "producer = Producer(**conf)\n",
    "\n",
    "# Kafka topic\n",
    "topic = \"pyspark-topic10\"\n",
    "\n",
    "# Path to user.avsc avro schema\n",
    "schema_path = \"/home/pyspark/pyspark_streaming/schema/schema_ver1.avsc\"\n",
    "schema = avro.schema.parse(open(schema_path).read())\n",
    "\n",
    "for i in range(1):\n",
    "    writer = avro.io.DatumWriter(schema)\n",
    "    bytes_writer = io.BytesIO()\n",
    "    encoder = avro.io.BinaryEncoder(bytes_writer)\n",
    "    # データの送信\n",
    "    writer.write({\"id\": \"yuki_schemaver1\",\n",
    "                    \"type\": \"login2\",\n",
    "                    \"sendtime\": random.randint(0, 10)}, encoder)\n",
    "    raw_bytes = bytes_writer.getvalue()\n",
    "    producer.produce(topic, raw_bytes, \"1\")\n",
    "producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 送信するpyspark \n",
    "# スキーマバージョン2で送信します\n",
    "\n",
    "conf = {'bootstrap.servers': 'kafka:9092'}\n",
    "producer_ver2 = Producer(**conf)\n",
    "\n",
    "# Kafka topic\n",
    "topic = \"pyspark-topic10\"\n",
    "\n",
    "# Path to user.avsc avro schema\n",
    "schema_path_ver2 = \"/home/pyspark/pyspark_streaming/schema/schema_ver2.avsc\"\n",
    "schema_ver2 = avro.schema.parse(open(schema_path_ver2).read())\n",
    "\n",
    "for i in range(1):\n",
    "    writer = avro.io.DatumWriter(schema_ver2)\n",
    "    bytes_writer = io.BytesIO()\n",
    "    encoder = avro.io.BinaryEncoder(bytes_writer)\n",
    "    # データの送信\n",
    "    writer.write({\n",
    "                    \"id\": \"ver2_yuki_schema\",\n",
    "                    \"type\": \"login2\",\n",
    "                    \"sendtime\": random.randint(0, 10),\n",
    "                    \"temp\": \"1000\"\n",
    "                }, encoder)\n",
    "    raw_bytes = bytes_writer.getvalue()\n",
    "    producer_ver2.produce(topic, raw_bytes, \"2\")\n",
    "producer_ver2.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark streaming(read avro) -> memory\n",
    "# スキーマバージョン2でデータ読み取ります(後方互換です。)\n",
    "from pyspark.sql.avro.functions import from_avro, to_avro\n",
    "\n",
    "memory_stream_check = df \\\n",
    "  .select(df.key,from_avro(\"value\", avro_json_schema_ver2, {\"mode\" : \"PERMISSIVE\"}).alias(\"json_col\")) \\\n",
    "  .writeStream \\\n",
    "  .trigger(processingTime=\"5 seconds\") \\\n",
    "  .format(\"memory\") \\\n",
    "  .queryName(\"ensyu2\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kafka/backword_check_ensyu2/\") \\\n",
    "  .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from ensyu2\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "578f5f657c2fb65ecadb997ad60e5cf2da380ecec34305a6dd913dc5b96e257c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('3.9.1': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
